# Ollama 壓力測試工具

這是一個基於Flask + Bootstrap 5的Ollama模型壓力測試網頁應用，可以幫助您測試不同Ollama模型在您系統上的性能表現，並提供詳細的硬體資訊和測試結果分析。

## 功能特色

- **硬體資訊檢測**: 頁面載入時自動檢測並直接顯示CPU、記憶體、GPU、系統等硬體資訊
- **模型管理**: 自動獲取本機可用的Ollama模型列表
- **壓力測試**: 支援多線程並發測試，可自定義並發數和總請求數
- **即時進度**: 使用AJAX輪詢技術提供即時的測試進度更新
- **詳細統計**: 提供完整的測試統計資訊，包括回應時間、成功率等
- **響應式設計**: 使用Bootstrap 5設計，支援各種螢幕尺寸
- **智能控制**: 測試期間自動禁用相關控制項，防止重複測試和配置修改

## 系統需求

- Python 3.8+
- Ollama 服務器正在運行
- 支援的作業系統: Windows, Linux, macOS

## 安裝步驟

1. **克隆或下載專案**
   ```bash
   git clone <repository-url>
   cd llmstresstest
   ```

2. **安裝依賴**
   ```bash
   pip install -r requirements.txt
   ```

3. **確保Ollama服務器運行**
   ```bash
   ollama serve
   ```

4. **啟動應用**
   ```bash
   python app_simple.py
   ```

5. **開啟瀏覽器**
   訪問 `http://localhost:5001`

## 使用方法

### 1. 查看硬體資訊
- 頁面載入時會直接顯示系統硬體資訊
- 包括CPU、記憶體、GPU、系統、網路資訊等
- 可點擊「重新整理」按鈕動態更新資訊

### 2. 配置測試參數
- **選擇模型**: 從下拉選單中選擇要測試的Ollama模型
- **並發請求數**: 設定同時進行的請求數量 (1-20)
- **總請求數**: 設定測試的總請求數量 (1-1000)
- **測試提示詞**: 輸入要測試的提示詞內容

### 3. 執行測試
- 點擊「開始測試」按鈕啟動測試
- 測試開始後，按鈕會立即變為「🔄 測試中...」並禁用，防止重複測試
- 同時所有表單控制項也會被禁用以防止修改配置
- 測試過程中會顯示即時進度和統計資訊
- 可隨時點擊「停止測試」按鈕中止測試，按鈕會暫時顯示「🔄 停止中...」
- 測試完成或停止後，所有控制項會自動恢復正常狀態

### 4. 查看結果
- 測試完成後會顯示詳細的統計結果
- 包括成功率、回應時間統計、每秒請求數等
- 所有操作都會記錄在即時日誌中

## 檔案結構

```
llmstresstest/
├── app_simple.py              # 主應用程式 (簡化版)
├── hardware_info.py           # 硬體資訊檢測模組
├── ollama_client.py           # Ollama API客戶端
├── stress_test_simple.py      # 壓力測試管理器 (簡化版)
├── requirements.txt           # Python依賴列表
├── templates/
│   └── index_simple.html      # 主頁面模板
├── static/
│   └── js/
│       └── app_simple.js      # 前端JavaScript
└── README.md                  # 說明文件
```

## API端點

- `GET /` - 主頁面
- `GET /api/hardware` - 獲取硬體資訊
- `GET /api/models` - 獲取可用模型列表
- `POST /api/start_test` - 開始壓力測試
- `POST /api/stop_test` - 停止壓力測試
- `GET /api/test_status/<test_id>` - 獲取測試狀態

## 測試結果說明

### 統計指標
- **總請求數**: 執行的總請求數量
- **成功請求數**: 成功完成的請求數量
- **失敗請求數**: 失敗的請求數量
- **成功率**: 成功請求的百分比
- **最小/最大回應時間**: 最快和最慢的回應時間
- **平均回應時間**: 所有成功請求的平均回應時間
- **中位數回應時間**: 回應時間的中位數
- **標準差**: 回應時間的標準差
- **每秒請求數**: 平均每秒處理的請求數量

## 故障排除

### 常見問題

1. **Ollama服務器連接失敗**
   - 確保Ollama服務器正在運行: `ollama serve`
   - 檢查服務器地址是否正確 (預設: http://localhost:11434)

2. **沒有可用模型**
   - 確保已下載Ollama模型: `ollama pull <model-name>`
   - 檢查模型列表: `ollama list`

3. **硬體資訊顯示錯誤**
   - 某些硬體資訊可能需要管理員權限
   - GPU資訊需要安裝相應的驅動程式

4. **測試執行緩慢**
   - 降低並發請求數
   - 選擇較小的模型進行測試
   - 檢查系統資源使用情況

5. **端口被佔用**
   - 如果5001端口被佔用，可以修改app_simple.py中的端口號
   - 或者停止佔用端口的其他應用程式

## 開發說明

### 技術棧
- **後端**: Flask, Python
- **前端**: Bootstrap 5, JavaScript
- **硬體檢測**: psutil, GPUtil
- **HTTP客戶端**: requests

### 擴展功能
- 可以添加更多的硬體資訊檢測
- 支援更多的測試參數配置
- 添加測試結果的圖表顯示
- 支援測試結果的匯出功能

## 授權

本專案採用MIT授權條款。

## 貢獻

歡迎提交Issue和Pull Request來改進這個專案。

## 聯絡資訊

如有問題或建議，請通過GitHub Issues聯絡。
